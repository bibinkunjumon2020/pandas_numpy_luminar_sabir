<html>
<head>
<title>METADATA</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #a9b7c6;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
METADATA</font>
</center></td></tr></table>
<pre><span class="s0">Metadata-Version: 2.1</span>
<span class="s0">Name: charset-normalizer</span>
<span class="s0">Version: 2.0.12</span>
<span class="s0">Summary: The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet.</span>
<span class="s0">Home-page: https://github.com/ousret/charset_normalizer</span>
<span class="s0">Author: Ahmed TAHRI @Ousret</span>
<span class="s0">Author-email: ahmed.tahri@cloudnursery.dev</span>
<span class="s0">License: MIT</span>
<span class="s0">Project-URL: Bug Reports, https://github.com/Ousret/charset_normalizer/issues</span>
<span class="s0">Project-URL: Documentation, https://charset-normalizer.readthedocs.io/en/latest</span>
<span class="s0">Keywords: encoding,i18n,txt,text,charset,charset-detector,normalization,unicode,chardet</span>
<span class="s0">Platform: UNKNOWN</span>
<span class="s0">Classifier: License :: OSI Approved :: MIT License</span>
<span class="s0">Classifier: Intended Audience :: Developers</span>
<span class="s0">Classifier: Topic :: Software Development :: Libraries :: Python Modules</span>
<span class="s0">Classifier: Operating System :: OS Independent</span>
<span class="s0">Classifier: Programming Language :: Python</span>
<span class="s0">Classifier: Programming Language :: Python :: 3</span>
<span class="s0">Classifier: Programming Language :: Python :: 3.5</span>
<span class="s0">Classifier: Programming Language :: Python :: 3.6</span>
<span class="s0">Classifier: Programming Language :: Python :: 3.7</span>
<span class="s0">Classifier: Programming Language :: Python :: 3.8</span>
<span class="s0">Classifier: Programming Language :: Python :: 3.9</span>
<span class="s0">Classifier: Programming Language :: Python :: 3.10</span>
<span class="s0">Classifier: Programming Language :: Python :: 3.11</span>
<span class="s0">Classifier: Topic :: Text Processing :: Linguistic</span>
<span class="s0">Classifier: Topic :: Utilities</span>
<span class="s0">Classifier: Programming Language :: Python :: Implementation :: PyPy</span>
<span class="s0">Classifier: Typing :: Typed</span>
<span class="s0">Requires-Python: &gt;=3.5.0</span>
<span class="s0">Description-Content-Type: text/markdown</span>
<span class="s0">License-File: LICENSE</span>
<span class="s0">Provides-Extra: unicode_backport</span>
<span class="s0">Requires-Dist: unicodedata2 ; extra == 'unicode_backport'</span>


<span class="s0">&lt;h1 align=&quot;center&quot;&gt;Charset Detection, for Everyone üëã &lt;a href=&quot;https://twitter.com/intent/tweet?text=The%20Real%20First%20Universal%20Charset%20%26%20Language%20Detector&amp;url=https://www.github.com/Ousret/charset_normalizer&amp;hashtags=python,encoding,chardet,developers&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/url/http/shields.io.svg?style=social&quot;/&gt;&lt;/a&gt;&lt;/h1&gt;</span>

<span class="s0">&lt;p align=&quot;center&quot;&gt;</span>
  <span class="s0">&lt;sup&gt;The Real First Universal Charset Detector&lt;/sup&gt;&lt;br&gt;</span>
  <span class="s0">&lt;a href=&quot;https://pypi.org/project/charset-normalizer&quot;&gt;</span>
    <span class="s0">&lt;img src=&quot;https://img.shields.io/pypi/pyversions/charset_normalizer.svg?orange=blue&quot; /&gt;</span>
  <span class="s0">&lt;/a&gt;</span>
  <span class="s0">&lt;a href=&quot;https://codecov.io/gh/Ousret/charset_normalizer&quot;&gt;</span>
      <span class="s0">&lt;img src=&quot;https://codecov.io/gh/Ousret/charset_normalizer/branch/master/graph/badge.svg&quot; /&gt;</span>
  <span class="s0">&lt;/a&gt;</span>
  <span class="s0">&lt;a href=&quot;https://pepy.tech/project/charset-normalizer/&quot;&gt;</span>
    <span class="s0">&lt;img alt=&quot;Download Count Total&quot; src=&quot;https://pepy.tech/badge/charset-normalizer/month&quot; /&gt;</span>
  <span class="s0">&lt;/a&gt;</span>
<span class="s0">&lt;/p&gt;</span>

<span class="s0">&gt; A library that helps you read text from an unknown charset encoding.&lt;br /&gt; Motivated by `chardet`,</span>
<span class="s0">&gt; I'm trying to resolve the issue by taking a new approach.</span>
<span class="s0">&gt; All IANA character set names for which the Python core library provides codecs are supported.</span>

<span class="s0">&lt;p align=&quot;center&quot;&gt;</span>
  <span class="s0">&gt;&gt;&gt;&gt;&gt; &lt;a href=&quot;https://charsetnormalizerweb.ousret.now.sh&quot; target=&quot;_blank&quot;&gt;üëâ Try Me Online Now, Then Adopt Me üëà &lt;/a&gt; &lt;&lt;&lt;&lt;&lt;</span>
<span class="s0">&lt;/p&gt;</span>

<span class="s0">This project offers you an alternative to **Universal Charset Encoding Detector**, also known as **Chardet**.</span>

<span class="s0">| Feature       | [Chardet](https://github.com/chardet/chardet)       | Charset Normalizer | [cChardet](https://github.com/PyYoshi/cChardet) |</span>
<span class="s0">| ------------- | :-------------: | :------------------: | :------------------: |</span>
<span class="s0">| `Fast`         | ‚ùå&lt;br&gt;          | ‚úÖ&lt;br&gt;             | ‚úÖ &lt;br&gt; |</span>
<span class="s0">| `Universal**`     | ‚ùå            | ‚úÖ                 | ‚ùå |</span>
<span class="s0">| `Reliable` **without** distinguishable standards | ‚ùå | ‚úÖ | ‚úÖ |</span>
<span class="s0">| `Reliable` **with** distinguishable standards | ‚úÖ | ‚úÖ | ‚úÖ |</span>
<span class="s0">| `Free &amp; Open`  | ‚úÖ             | ‚úÖ                | ‚úÖ |</span>
<span class="s0">| `License` | LGPL-2.1 | MIT | MPL-1.1</span>
<span class="s0">| `Native Python` | ‚úÖ | ‚úÖ | ‚ùå |</span>
<span class="s0">| `Detect spoken language` | ‚ùå | ‚úÖ | N/A |</span>
<span class="s0">| `Supported Encoding` | 30 | :tada: [93](https://charset-normalizer.readthedocs.io/en/latest/user/support.html#supported-encodings)  | 40</span>

<span class="s0">&lt;p align=&quot;center&quot;&gt;</span>
<span class="s0">&lt;img src=&quot;https://i.imgflip.com/373iay.gif&quot; alt=&quot;Reading Normalized Text&quot; width=&quot;226&quot;/&gt;&lt;img src=&quot;https://media.tenor.com/images/c0180f70732a18b4965448d33adba3d0/tenor.gif&quot; alt=&quot;Cat Reading Text&quot; width=&quot;200&quot;/&gt;</span>

<span class="s0">*\*\* : They are clearly using specific code for a specific encoding even if covering most of used one*&lt;br&gt; </span>
<span class="s0">Did you got there because of the logs? See [https://charset-normalizer.readthedocs.io/en/latest/user/miscellaneous.html](https://charset-normalizer.readthedocs.io/en/latest/user/miscellaneous.html)</span>

<span class="s0">## ‚≠ê Your support</span>

<span class="s0">*Fork, test-it, star-it, submit your ideas! We do listen.*</span>
  
<span class="s0">## ‚ö° Performance</span>

<span class="s0">This package offer better performance than its counterpart Chardet. Here are some numbers.</span>

<span class="s0">| Package       | Accuracy       | Mean per file (ms) | File per sec (est) |</span>
<span class="s0">| ------------- | :-------------: | :------------------: | :------------------: |</span>
<span class="s0">|      [chardet](https://github.com/chardet/chardet)        |     92 %     |     220 ms      |       5 file/sec        |</span>
<span class="s0">| charset-normalizer |    **98 %**     |     **40 ms**      |       25 file/sec    |</span>

<span class="s0">| Package       | 99th percentile       | 95th percentile | 50th percentile |</span>
<span class="s0">| ------------- | :-------------: | :------------------: | :------------------: |</span>
<span class="s0">|      [chardet](https://github.com/chardet/chardet)        |     1115 ms     |     300 ms      |       27 ms        |</span>
<span class="s0">| charset-normalizer |    460 ms     |     240 ms      |       18 ms    |</span>

<span class="s0">Chardet's performance on larger file (1MB+) are very poor. Expect huge difference on large payload.</span>

<span class="s0">&gt; Stats are generated using 400+ files using default parameters. More details on used files, see GHA workflows.</span>
<span class="s0">&gt; And yes, these results might change at any time. The dataset can be updated to include more files.</span>
<span class="s0">&gt; The actual delays heavily depends on your CPU capabilities. The factors should remain the same.</span>

<span class="s0">[cchardet](https://github.com/PyYoshi/cChardet) is a non-native (cpp binding) and unmaintained faster alternative with </span>
<span class="s0">a better accuracy than chardet but lower than this package. If speed is the most important factor, you should try it.</span>

<span class="s0">## ‚ú® Installation</span>

<span class="s0">Using PyPi for latest stable</span>
<span class="s0">```sh</span>
<span class="s0">pip install charset-normalizer -U</span>
<span class="s0">```</span>

<span class="s0">If you want a more up-to-date `unicodedata` than the one available in your Python setup.</span>
<span class="s0">```sh</span>
<span class="s0">pip install charset-normalizer[unicode_backport] -U</span>
<span class="s0">```</span>

<span class="s0">## üöÄ Basic Usage</span>

<span class="s0">### CLI</span>
<span class="s0">This package comes with a CLI.</span>

<span class="s0">```</span>
<span class="s0">usage: normalizer [-h] [-v] [-a] [-n] [-m] [-r] [-f] [-t THRESHOLD]</span>
                  <span class="s0">file [file ...]</span>

<span class="s0">The Real First Universal Charset Detector. Discover originating encoding used</span>
<span class="s0">on text file. Normalize text to unicode.</span>

<span class="s0">positional arguments:</span>
  <span class="s0">files                 File(s) to be analysed</span>

<span class="s0">optional arguments:</span>
  <span class="s0">-h, --help            show this help message and exit</span>
  <span class="s0">-v, --verbose         Display complementary information about file if any.</span>
                        <span class="s0">Stdout will contain logs about the detection process.</span>
  <span class="s0">-a, --with-alternative</span>
                        <span class="s0">Output complementary possibilities if any. Top-level</span>
                        <span class="s0">JSON WILL be a list.</span>
  <span class="s0">-n, --normalize       Permit to normalize input file. If not set, program</span>
                        <span class="s0">does not write anything.</span>
  <span class="s0">-m, --minimal         Only output the charset detected to STDOUT. Disabling</span>
                        <span class="s0">JSON output.</span>
  <span class="s0">-r, --replace         Replace file when trying to normalize it instead of</span>
                        <span class="s0">creating a new one.</span>
  <span class="s0">-f, --force           Replace file without asking if you are sure, use this</span>
                        <span class="s0">flag with caution.</span>
  <span class="s0">-t THRESHOLD, --threshold THRESHOLD</span>
                        <span class="s0">Define a custom maximum amount of chaos allowed in</span>
                        <span class="s0">decoded content. 0. &lt;= chaos &lt;= 1.</span>
  <span class="s0">--version             Show version information and exit.</span>
<span class="s0">```</span>

<span class="s0">```bash</span>
<span class="s0">normalizer ./data/sample.1.fr.srt</span>
<span class="s0">```</span>

<span class="s0">:tada: Since version 1.4.0 the CLI produce easily usable stdout result in JSON format.</span>

<span class="s0">```json</span>
<span class="s0">{</span>
    <span class="s0">&quot;path&quot;: &quot;/home/default/projects/charset_normalizer/data/sample.1.fr.srt&quot;,</span>
    <span class="s0">&quot;encoding&quot;: &quot;cp1252&quot;,</span>
    <span class="s0">&quot;encoding_aliases&quot;: [</span>
        <span class="s0">&quot;1252&quot;,</span>
        <span class="s0">&quot;windows_1252&quot;</span>
    <span class="s0">],</span>
    <span class="s0">&quot;alternative_encodings&quot;: [</span>
        <span class="s0">&quot;cp1254&quot;,</span>
        <span class="s0">&quot;cp1256&quot;,</span>
        <span class="s0">&quot;cp1258&quot;,</span>
        <span class="s0">&quot;iso8859_14&quot;,</span>
        <span class="s0">&quot;iso8859_15&quot;,</span>
        <span class="s0">&quot;iso8859_16&quot;,</span>
        <span class="s0">&quot;iso8859_3&quot;,</span>
        <span class="s0">&quot;iso8859_9&quot;,</span>
        <span class="s0">&quot;latin_1&quot;,</span>
        <span class="s0">&quot;mbcs&quot;</span>
    <span class="s0">],</span>
    <span class="s0">&quot;language&quot;: &quot;French&quot;,</span>
    <span class="s0">&quot;alphabets&quot;: [</span>
        <span class="s0">&quot;Basic Latin&quot;,</span>
        <span class="s0">&quot;Latin-1 Supplement&quot;</span>
    <span class="s0">],</span>
    <span class="s0">&quot;has_sig_or_bom&quot;: false,</span>
    <span class="s0">&quot;chaos&quot;: 0.149,</span>
    <span class="s0">&quot;coherence&quot;: 97.152,</span>
    <span class="s0">&quot;unicode_path&quot;: null,</span>
    <span class="s0">&quot;is_preferred&quot;: true</span>
<span class="s0">}</span>
<span class="s0">```</span>

<span class="s0">### Python</span>
<span class="s0">*Just print out normalized text*</span>
<span class="s0">```python</span>
<span class="s0">from charset_normalizer import from_path</span>

<span class="s0">results = from_path('./my_subtitle.srt')</span>

<span class="s0">print(str(results.best()))</span>
<span class="s0">```</span>

<span class="s0">*Normalize any text file*</span>
<span class="s0">```python</span>
<span class="s0">from charset_normalizer import normalize</span>
<span class="s0">try:</span>
    <span class="s0">normalize('./my_subtitle.srt') # should write to disk my_subtitle-***.srt</span>
<span class="s0">except IOError as e:</span>
    <span class="s0">print('Sadly, we are unable to perform charset normalization.', str(e))</span>
<span class="s0">```</span>

<span class="s0">*Upgrade your code without effort*</span>
<span class="s0">```python</span>
<span class="s0">from charset_normalizer import detect</span>
<span class="s0">```</span>

<span class="s0">The above code will behave the same as **chardet**. We ensure that we offer the best (reasonable) BC result possible.</span>

<span class="s0">See the docs for advanced usage : [readthedocs.io](https://charset-normalizer.readthedocs.io/en/latest/)</span>

<span class="s0">## üòá Why</span>

<span class="s0">When I started using Chardet, I noticed that it was not suited to my expectations, and I wanted to propose a</span>
<span class="s0">reliable alternative using a completely different method. Also! I never back down on a good challenge!</span>

<span class="s0">I **don't care** about the **originating charset** encoding, because **two different tables** can</span>
<span class="s0">produce **two identical rendered string.**</span>
<span class="s0">What I want is to get readable text, the best I can. </span>

<span class="s0">In a way, **I'm brute forcing text decoding.** How cool is that ? üòé</span>

<span class="s0">Don't confuse package **ftfy** with charset-normalizer or chardet. ftfy goal is to repair unicode string whereas charset-normalizer to convert raw file in unknown encoding to unicode.</span>

<span class="s0">## üç∞ How</span>

  <span class="s0">- Discard all charset encoding table that could not fit the binary content.</span>
  <span class="s0">- Measure chaos, or the mess once opened (by chunks) with a corresponding charset encoding.</span>
  <span class="s0">- Extract matches with the lowest mess detected.</span>
  <span class="s0">- Additionally, we measure coherence / probe for a language.</span>

<span class="s0">**Wait a minute**, what is chaos/mess and coherence according to **YOU ?**</span>

<span class="s0">*Chaos :* I opened hundred of text files, **written by humans**, with the wrong encoding table. **I observed**, then</span>
<span class="s0">**I established** some ground rules about **what is obvious** when **it seems like** a mess.</span>
 <span class="s0">I know that my interpretation of what is chaotic is very subjective, feel free to contribute in order to</span>
 <span class="s0">improve or rewrite it.</span>

<span class="s0">*Coherence :* For each language there is on earth, we have computed ranked letter appearance occurrences (the best we can). So I thought</span>
<span class="s0">that intel is worth something here. So I use those records against decoded text to check if I can detect intelligent design.</span>

<span class="s0">## ‚ö° Known limitations</span>

  <span class="s0">- Language detection is unreliable when text contains two or more languages sharing identical letters. (eg. HTML (english tags) + Turkish content (Sharing Latin characters))</span>
  <span class="s0">- Every charset detector heavily depends on sufficient content. In common cases, do not bother run detection on very tiny content.</span>

<span class="s0">## üë§ Contributing</span>

<span class="s0">Contributions, issues and feature requests are very much welcome.&lt;br /&gt;</span>
<span class="s0">Feel free to check [issues page](https://github.com/ousret/charset_normalizer/issues) if you want to contribute.</span>

<span class="s0">## üìù License</span>

<span class="s0">Copyright ¬© 2019 [Ahmed TAHRI @Ousret](https://github.com/Ousret).&lt;br /&gt;</span>
<span class="s0">This project is [MIT](https://github.com/Ousret/charset_normalizer/blob/master/LICENSE) licensed.</span>

<span class="s0">Characters frequencies used in this project ¬© 2012 [Denny Vrandeƒçiƒá](http://simia.net/letters/)</span>


</pre>
</body>
</html>